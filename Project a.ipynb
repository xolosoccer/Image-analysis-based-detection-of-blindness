{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import glob\nimport numpy as np\nimport os\nimport shutil\nfrom tqdm import tqdm\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.preprocessing import image\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.applications import VGG16, ResNet50, Xception\nfrom keras import optimizers\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.optimizers import RMSprop,SGD,Adagrad,Adadelta,Adam,Adamax,Nadam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers import BatchNormalization\nfrom keras import utils\nfrom keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\nimport h5py\nfrom keras.models import load_model\nfrom PIL import Image\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight, shuffle\nfrom sklearn.metrics import accuracy_score\n\nsns.set(style='white', context='notebook', palette='dark')\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\ntest_df = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\nprint(train_df.shape)\nprint(test_df.shape)\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.countplot(train_df['diagnosis'])\nsns.despine()\ntrain_df['diagnosis'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_samples(df, columns=2, rows=2):\n    fig=plt.figure(figsize=(5*columns, 4*rows))\n\n    for i in range(columns*rows):\n        image_path = df.loc[i,'id_code']\n        image_id = df.loc[i,'diagnosis']\n        img = cv2.imread(f'../input/aptos2019-blindness-detection/train_images/{image_path}.png')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        fig.add_subplot(rows, columns, i+1)\n        plt.title(image_id)\n        plt.imshow(img)\n    \n    plt.tight_layout()\n\ndisplay_samples(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing augmented data","metadata":{}},{"cell_type":"code","source":"orig_img = np.array(Image.open('../input/aptos2019-blindness-detection/test_images/351aba543dc8.png'))\nplt.imshow(orig_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\ndef generate_plot_pics(datagen,orig_img):\n    dir_augmented_data = \"/kaggle/working/aug_preview\"\n    try:\n        ## if the preview folder does not exist, create\n        os.mkdir(dir_augmented_data)\n    except:\n        ## if the preview folder exists, then remove\n        ## the contents (pictures) in the folder\n        for item in os.listdir(dir_augmented_data):\n            os.remove(dir_augmented_data + \"/\" + item)\n    ## convert the original image to array\n    x = img_to_array(orig_img)\n    ## reshape (Sampke, Nrow, Ncol, 3) 3 = R, G or B\n    x = x.reshape((1,) + x.shape)\n    ## -------------------------- ##\n    ## randomly generate pictures\n    ## -------------------------- ##\n    i = 0\n    Nplot = 8\n    for batch in datagen.flow(x,batch_size=1,\n                        save_to_dir=dir_augmented_data,\n                        save_prefix=\"pic\",\n                        save_format='png'):\n        i += 1\n        if i > Nplot - 1: ## generate 8 pictures\n            break\n    ## -------------------------- ##\n    ## plot the generated data\n    ## -------------------------- ##\n    fig = plt.figure(figsize=(8, 6))\n    fig.subplots_adjust(hspace=0.02,wspace=0.01,\n    left=0,right=1,bottom=0, top=1)\n    ## original picture\n    ax = fig.add_subplot(3, 3, 1,xticks=[],yticks=[])\n    ax.imshow(orig_img)\n    ax.set_title(\"original\")\n    i = 2\n    for imgnm in os.listdir(dir_augmented_data):\n        ax = fig.add_subplot(3, 3, i,xticks=[],yticks=[])\n        img = load_img(dir_augmented_data + \"/\" + imgnm)\n        ax.imshow(img)\n        i += 1\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## rotation_range: Int. Degree range for random rotations.\ndatagen = ImageDataGenerator(rotation_range=30)\ngenerate_plot_pics(datagen,orig_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## zoom_range: Float or [lower, upper]. Range for random zoom.\n## If a float, [lower, upper] = [1-zoom_range, 1+zoom_range].\ndatagen = ImageDataGenerator(zoom_range=0.1)\ngenerate_plot_pics(datagen,orig_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(vertical_flip=True)\ngenerate_plot_pics(datagen,orig_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pad_width(im, new_shape, is_rgb=True):\n    pad_diff = new_shape - im.shape[0], new_shape - im.shape[1]\n    t, b = math.floor(pad_diff[0]/2), math.ceil(pad_diff[0]/2) # padding the top (t) and bottom (b) of images\n    l, r = math.floor(pad_diff[1]/2), math.ceil(pad_diff[1]/2) # padding the left (l) and right (r) of images\n    if is_rgb:\n        pad_width = ((t,b), (l,r), (0, 0))\n    else:\n        pad_width = ((t,b), (l,r))\n    return pad_width\n\ndef preprocess_image(image_path, desired_size=224): #choosing image size\n    im = Image.open(image_path)\n    im = im.resize((desired_size, )*2, resample=Image.LANCZOS)\n    \n    return im","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = train_df.shape[0]\nx_train = np.empty((N, 224, 224, 3), dtype=np.uint8) #converting data into a numpy array\n\nfor i, image_id in enumerate(tqdm(train_df['id_code'])):\n    x_train[i, :, :, :] = preprocess_image(\n        f'../input/aptos2019-blindness-detection/train_images/{image_id}.png'\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = test_df.shape[0]\nx_test = np.empty((N, 224, 224, 3), dtype=np.uint8)\n\nfor i, image_id in enumerate(tqdm(test_df['id_code'])):\n    x_test[i, :, :, :] = preprocess_image(\n        f'../input/aptos2019-blindness-detection/test_images/{image_id}.png'\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_train = pd.get_dummies(train_df['diagnosis']).values\ny_train = keras.utils.to_categorical(train_df['diagnosis'], num_classes=5, dtype='float32')\nprint(x_train.shape)\nprint(y_train.shape) \n#print(x_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting data for train and validation \nx_train, x_val, y_train, y_val = train_test_split(\n    x_train, y_train, \n    test_size=0.2, \n    random_state=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normalizing data\nx_train = x_train / 255\nx_val = x_val / 255\n#x_test = x_test / 255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_datagen():\n    return ImageDataGenerator(\n        rotation_range=30, # set range for random rotation in degrees\n        zoom_range=0.10,  # set range for random zoom \n        # set mode for filling points outside the input boundaries\n        fill_mode='constant',\n        cval=0.,  # value used for fill_mode = \"constant\"\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=True,  # randomly flip images\n    )\ndata_generator = create_datagen().flow(x_train, y_train, batch_size=30, seed=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 30\nnum_classes = 5\nepochs = 10\ninput_shape = (224, 224, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.optimizers import RMSprop,SGD,Adagrad,Adadelta,Adam,Adamax,Nadam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers import BatchNormalization\nimport h5py\nfrom keras.models import load_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 convolution layers (conv-pool-conv-pool)","metadata":{}},{"cell_type":"code","source":"model_a = Sequential()\n\nmodel_a.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_a.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_a.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_a.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_a.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_a.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_a.add(Flatten())\nmodel_a.add(Dense(512, activation='relu'))\nmodel_a.add(Dense(5, activation='softmax'))\n\n\nmodel_a.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_a.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_a.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 3 convolution layers, Adam optimizer, and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 convolution layers (conv-conv-pool-conv-conv-pool)","metadata":{}},{"cell_type":"code","source":"model_b = Sequential()\n\nmodel_b.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_b.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_b.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_b.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_b.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_b.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_b.add(Flatten())\nmodel_b.add(Dense(512, activation='relu'))\nmodel_b.add(Dense(5, activation='softmax'))\n\n\nmodel_b.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_b.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_b.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 3 convolution layers, Adam optimizer, and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Determining optimal number of layers\n","metadata":{}},{"cell_type":"code","source":"model_b = Sequential()\n\nmodel_b.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_b.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_b.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_b.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_b.add(Flatten())\nmodel_b.add(Dense(512, activation='relu'))\nmodel_b.add(Dense(5, activation='softmax'))\n\n\nmodel_b.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_b.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_b.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 4 convolution layers, Adam optimizer, and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_c = Sequential()\n\nmodel_c.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_c.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_c.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_c.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_c.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_c.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_c.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_c.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_c.add(Conv2D(filters = 256, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_c.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_c.add(Flatten())\nmodel_c.add(Dense(512, activation='relu'))\nmodel_c.add(Dense(5, activation='softmax'))\n\n\nmodel_c.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_c.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_c.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 5 convolution layers, Adam optimizer, and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_d = Sequential()\n\nmodel_d.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Conv2D(filters = 256, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Conv2D(filters = 512, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Flatten())\nmodel_d.add(Dense(512, activation='relu'))\nmodel_d.add(Dense(5, activation='softmax'))\n\n\nmodel_d.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_d.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_d.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 6 convolution layers, Adam optimizer, and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_d = Sequential()\n\n# model_d.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n#                  input_shape=input_shape))\n# model_d.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n#                  input_shape=input_shape))\n# model_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Conv2D(filters = 256, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(Conv2D(filters = 256, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Conv2D(filters = 512, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(Conv2D(filters = 512, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_d.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_d.add(Flatten())\nmodel_d.add(Dense(512, activation='relu'))\nmodel_d.add(Dense(5, activation='softmax'))\n\n\nmodel_d.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_d.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_d.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Selecting best optimizer","metadata":{}},{"cell_type":"markdown","source":"# 1b. Simple model using 1 convolutional layer and Adam optimizer","metadata":{}},{"cell_type":"code","source":"model_1b = Sequential()\n\nmodel_1b.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_1b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_1b.add(Flatten())\nmodel_1b.add(Dense(512, activation='relu'))\nmodel_1b.add(Dense(5, activation='softmax'))\n\nmodel_1b.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_1b.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_1b.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Single-layer CNN Performance with Adam optimizer and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2b. Simple model using 2 convolutional layers and Adam optimizer","metadata":{}},{"cell_type":"code","source":"model_2b = Sequential()\n\nmodel_2b.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_2b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_2b.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_2b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_2b.add(Flatten())\nmodel_2b.add(Dense(512, activation='relu'))\nmodel_2b.add(Dense(5, activation='relu'))\n\n\nmodel_2b.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_2b.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_2b.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Two-layer CNN Performance with Adam optimizer and relu activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2b2 = Sequential()\n\nmodel_2b2.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_2b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_2b2.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_2b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_2b2.add(Flatten())\nmodel_2b2.add(Dense(512, activation='relu'))\nmodel_2b2.add(Dense(5, activation='softmax'))\n\nmodel_2b2.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_2b2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_2b2.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Two-layer CNN Performance with Adam optimizer and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3b. Simple model using 3 convolutional layers and Adam optimizer","metadata":{}},{"cell_type":"code","source":"model_3b = Sequential()\n\nmodel_3b.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_3b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_3b.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_3b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_3b.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_3b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_3b.add(Flatten())\nmodel_3b.add(Dense(512, activation='relu'))\nmodel_3b.add(Dense(5, activation='relu'))\n\n\nmodel_3b.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_3b.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_3b.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Three-layer CNN Performance with Adam optimizer and relu activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3b2 = Sequential()\n\nmodel_3b2.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_3b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_3b2.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_3b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_3b2.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_3b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_3b2.add(Flatten())\nmodel_3b2.add(Dense(512, activation='relu'))\nmodel_3b2.add(Dense(5, activation='softmax'))\n\nmodel_3b2.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_3b2.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_3b2.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Three-layer CNN Performance with Adam optimizer and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4b. CNN model using 3 convolutional layers with regularization using dropout (0.1) and Adam optimizer","metadata":{}},{"cell_type":"code","source":"model_4b = Sequential()\n\nmodel_4b.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_4b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_4b.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_4b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_4b.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_4b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_4b.add(Flatten())\nmodel_4b.add(Dense(512, activation='relu'))\nmodel_4b.add(Dropout(0.1))\nmodel_4b.add(Dense(512, activation='relu'))\nmodel_4b.add(Dropout(0.1))\nmodel_4b.add(Dense(5, activation='relu'))\n\n\nmodel_4b.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_4b.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_4b.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Three-layer CNN Performance with 0.1 dropout with Adam optimizer and relu activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_4b2 = Sequential()\n\nmodel_4b2.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_4b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_4b2.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_4b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_4b2.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_4b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_4b2.add(Flatten())\nmodel_4b2.add(Dense(512, activation='relu'))\nmodel_4b2.add(Dropout(0.1))\nmodel_4b2.add(Dense(512, activation='relu'))\nmodel_4b2.add(Dropout(0.1))\nmodel_4b2.add(Dense(5, activation='softmax'))\n\nmodel_4b2.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_4b2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_4b2.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Three-layer CNN Performance with 0.1 dropout with Adam optimizer and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5b. CNN model using 3 convolutional layers with regularization using dropout (0.3) and Adam optimizer","metadata":{}},{"cell_type":"code","source":"model_5b = Sequential()\n\nmodel_5b.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_5b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_5b.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_5b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_5b.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_5b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_5b.add(Flatten())\nmodel_5b.add(Dense(512, activation='relu'))\nmodel_5b.add(Dropout(0.3))\nmodel_5b.add(Dense(512, activation='relu'))\nmodel_5b.add(Dropout(0.3))\nmodel_5b.add(Dense(5, activation='relu'))\n\n\nmodel_5b.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_5b.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_5b.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Three-layer CNN Performance with 0.3 dropout with Adam optimizer and relu activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_5b2 = Sequential()\n\nmodel_5b2.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_5b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_5b2.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_5b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_5b2.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_5b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_5b2.add(Flatten())\nmodel_5b2.add(Dense(512, activation='relu'))\nmodel_5b2.add(Dropout(0.3))\nmodel_5b2.add(Dense(512, activation='relu'))\nmodel_5b2.add(Dropout(0.3))\nmodel_5b2.add(Dense(5, activation='softmax'))\n\nmodel_5b2.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_5b2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_5b2.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Three-layer CNN Performance with 0.3 dropout with Adam optimizer and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6b. CNN model using 3 convolutional layers with regularization using dropout (0.5) and Adam optimizer","metadata":{}},{"cell_type":"code","source":"model_6b = Sequential()\n\nmodel_6b.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_6b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_6b.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_6b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_6b.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_6b.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_6b.add(Flatten())\n# model_6b.add(Dense(512, activation='relu'))\n# model_6b.add(Dropout(0.5))\nmodel_6b.add(Dense(512, activation='relu'))\nmodel_6b.add(Dropout(0.5))\n#model_6b.add(Dense(5, activation='relu'))\nmodel_6b.add(Dense(5, activation='softmax'))\n\n\nmodel_6b.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_6b.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_6b.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Three-layer CNN Performance with 0.5 dropout with Adam optimizer and relu activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_6b2 = Sequential()\n\nmodel_6b2.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_6b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_6b2.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_6b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_6b2.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_6b2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_6b2.add(Flatten())\nmodel_6b2.add(Dense(512, activation='relu'))\nmodel_6b2.add(Dropout(0.5))\nmodel_6b2.add(Dense(512, activation='relu'))\nmodel_6b2.add(Dropout(0.5))\nmodel_6b2.add(Dense(5, activation='softmax'))\n\nmodel_6b2.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_6b2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_6b2.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Three-layer CNN Performance with 0.5 dropout with Adam optimizer and softmax activation', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine tuning best model","metadata":{}},{"cell_type":"markdown","source":"## Modifying kernel size","metadata":{}},{"cell_type":"code","source":"model_k33 = Sequential()\n\nmodel_k33.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_k33.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k33.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_k33.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k33.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_k33.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k33.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_k33.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k33.add(Flatten())\nmodel_k33.add(Dense(512, activation='relu'))\nmodel_k33.add(Dense(5, activation='softmax'))\n\n\nmodel_k33.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_k33.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_k33.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 4 convolution layers, 3x3 kernel size', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_k55 = Sequential()\n\nmodel_k55.add(Conv2D(filters = 16, kernel_size=(5, 5), activation='relu', \n                 input_shape=input_shape))\nmodel_k55.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k55.add(Conv2D(filters = 32, kernel_size=(5, 5), activation='relu', \n                 input_shape=input_shape))\nmodel_k55.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k55.add(Conv2D(filters = 64, kernel_size=(5, 5), activation='relu', \n                 input_shape=input_shape))\nmodel_k55.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k55.add(Conv2D(filters = 128, kernel_size=(5, 5), activation='relu', \n                 input_shape=input_shape))\nmodel_k55.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k55.add(Flatten())\nmodel_k55.add(Dense(512, activation='relu'))\nmodel_k55.add(Dense(5, activation='softmax'))\n\n\nmodel_k55.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_k55.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_k55.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 4 convolution layers, 5x5 kernel size', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_k77 = Sequential()\n\nmodel_k77.add(Conv2D(filters = 16, kernel_size=(7, 7), activation='relu', \n                 input_shape=input_shape))\nmodel_k77.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k77.add(Conv2D(filters = 32, kernel_size=(7, 7), activation='relu', \n                 input_shape=input_shape))\nmodel_k77.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k77.add(Conv2D(filters = 64, kernel_size=(7, 7), activation='relu', \n                 input_shape=input_shape))\nmodel_k77.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k77.add(Conv2D(filters = 128, kernel_size=(7, 7), activation='relu', \n                 input_shape=input_shape))\nmodel_k77.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_k77.add(Flatten())\nmodel_k77.add(Dense(512, activation='relu'))\nmodel_k77.add(Dense(5, activation='softmax'))\n\n\nmodel_k77.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_k77.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_k77.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 4 convolution layers, 7x7 kernel size', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,10))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using additional epochs","metadata":{}},{"cell_type":"code","source":"batch_size = 30\nnum_classes = 5\nepochs = 20\ninput_shape = (224, 224, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_e50 = Sequential()\n\nmodel_e50.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_e50.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_e50.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_e50.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_e50.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_e50.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_e50.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_e50.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_e50.add(Flatten())\nmodel_e50.add(Dense(512, activation='relu'))\nmodel_e50.add(Dense(5, activation='softmax'))\n\n\nmodel_e50.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_e50.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_e50.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 4 convolution layers, 3x3 kernel size, 50 epochs', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,20))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 50, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 50, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using batch normalization","metadata":{}},{"cell_type":"code","source":"model_bn = Sequential()\n\nmodel_bn.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_bn.add(BatchNormalization())\nmodel_bn.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_bn.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_bn.add(BatchNormalization())\nmodel_bn.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_bn.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_bn.add(BatchNormalization())\nmodel_bn.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_bn.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_bn.add(BatchNormalization())\nmodel_bn.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_bn.add(Flatten())\nmodel_bn.add(Dense(512, activation='relu'))\nmodel_bn.add(BatchNormalization())\nmodel_bn.add(Dense(5, activation='softmax'))\n\n\nmodel_bn.compile(loss='categorical_crossentropy',\n              #optimizer=optimizers.Adam(),\n              #optimizer=optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False),\n              optimizer=optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True),\n              metrics=['accuracy'])\n\nmodel_bn.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_bn.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 4 convolution layers with batch normalization and learning rate = 0.01, amsgrad=True', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,20))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimizing dropout","metadata":{}},{"cell_type":"code","source":"model_drop = Sequential()\n\nmodel_drop.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_drop.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_drop.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_drop.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_drop.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_drop.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_drop.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_drop.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_drop.add(Flatten())\nmodel_drop.add(Dense(512, activation='relu'))\nmodel_drop.add(Dropout(0.1))\n#model_drop.add(Dropout(0.3))\n#model_drop.add(Dropout(0.5))\nmodel_drop.add(Dense(5, activation='softmax'))\n\n\nmodel_drop.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\n#model_drop.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_drop.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 4 convolution layers with dropout 0.1', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,20))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Batch normalization and dropout 0.5","metadata":{}},{"cell_type":"code","source":"model_bdrop = Sequential()\n\nmodel_bdrop.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_bdrop.add(BatchNormalization())\nmodel_bdrop.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_bdrop.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_bdrop.add(BatchNormalization())\nmodel_bdrop.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_bdrop.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_bdrop.add(BatchNormalization())\nmodel_bdrop.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_bdrop.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_bdrop.add(BatchNormalization())\nmodel_bdrop.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_bdrop.add(Flatten())\nmodel_bdrop.add(Dense(512, activation='relu'))\nmodel_bdrop.add(Dropout(0.5))\nmodel_bdrop.add(Dense(5, activation='softmax'))\n\n\nmodel_bdrop.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_bdrop.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_bdrop.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance with 4 convolution layers with batch normalization and dropout 0.5', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,20))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Top performing model","metadata":{}},{"cell_type":"code","source":"model_base = Sequential()\n\nmodel_base.add(Conv2D(filters = 16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_base.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_base.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_base.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_base.add(Conv2D(filters = 64, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_base.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_base.add(Conv2D(filters = 128, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\nmodel_base.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_base.add(Flatten())\nmodel_base.add(Dense(512, activation='relu'))\nmodel_base.add(Dropout(0.5))\nmodel_base.add(Dense(5, activation='softmax'))\n\n\nmodel_base.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel_base.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_base.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 30\nnum_classes = 5\nepochs = 20\ninput_shape = (224, 224, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transfer learning ","metadata":{}},{"cell_type":"markdown","source":"## ResNet50","metadata":{}},{"cell_type":"code","source":"batch_size = 30\nnum_classes = 5\nepochs = 20\ninput_shape = (224, 224, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.applications import ResNet50\n\nresnet50_base = ResNet50(weights = 'imagenet', include_top = False, input_shape = input_shape)\n\nresnet50_base.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import models\nfrom keras import layers\nresnet50_60_model = models.Sequential()\nresnet50_60_model.add(resnet50_base)\nresnet50_60_model.add(layers.Flatten())\nresnet50_60_model.add(layers.Dense(512, activation='relu'))\n#model.add(BatchNormalization())\nresnet50_60_model.add(layers.Dropout(0.6)) \nresnet50_60_model.add(layers.Dense(5, activation='softmax'))\n\nresnet50_60_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import models\nfrom keras import layers\nresnet50_model = models.Sequential()\nresnet50_model.add(resnet50_base)\nresnet50_model.add(layers.GlobalAveragePooling2D())\nresnet50_model.add(Dropout(0.5))\nresnet50_model.add(layers.Dense(5, activation='softmax'))\n\nresnet50_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('This is the number of trainable weights '\n'before freezing the conv base:', len(resnet50_60_model.trainable_weights))\n\nresnet50_base.trainable = False\nprint('This is the number of trainable weights '\n'after freezing the conv base:', len(resnet50_60_model.trainable_weights))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet50_60_model.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(learning_rate=0.00005),\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = resnet50_60_model.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance ResNet50 pretrained weights plus global average pooling', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,20))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 20, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 20, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv_base.trainable = True\nset_trainable = False\nfor layer in conv_base.layers:\n    if layer.name == 'res5a_branch2a':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet50_base.trainable = True\nset_trainable = False\nfor layer in resnet50_base.layers:\n    if layer.name == 'res4a_branch2a':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet50_60_model.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(learning_rate=0.00005),\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 30\nnum_classes = 5\nepochs = 20\ninput_shape = (224, 224, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = resnet50_60_model.fit_generator(\n    data_generator,\n    steps_per_epoch=x_train.shape[0] / batch_size,\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[#EarlyStopping(monitor = 'accuracy', patience=5, restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance ResNet50 pretrained weights with fine-tuning (2nd block) plus dropout 0.6', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(0,50))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 50, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 50, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nresnet50_60_model.save('/kaggle/working/resnet50_model.h5') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saved models","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nvgg16_model.save('/kaggle/working/vgg16_model.h5')  \nresnet50_model.save('/kaggle/working/resnet50_model.h5')\nxception_model.save('/kaggle/working/xception_model.h5')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing Filters & Feature Maps","metadata":{}},{"cell_type":"markdown","source":"## Filters","metadata":{}},{"cell_type":"code","source":"# cannot easily visualize filters lower down\nfrom keras.applications.vgg16 import VGG16\nfrom matplotlib import pyplot\n# load the model\nmodel = VGG16()\n# retrieve weights from the second hidden layer\nfilters, biases = model.layers[1].get_weights()\n# normalize filter values to 0-1 so we can visualize them\nf_min, f_max = filters.min(), filters.max()\nfilters = (filters - f_min) / (f_max - f_min)\n# plot first few filters\nn_filters, ix = 6, 1\nfor i in range(n_filters):\n    # get the filter\n    f = filters[:, :, :, i]\n    # plot each channel separately\n    for j in range(3):\n        # specify subplot and turn of axis\n        ax = pyplot.subplot(n_filters, 3, ix)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        # plot filter channel in grayscale\n        pyplot.imshow(f[:, :, j], cmap='gray')\n        ix += 1\n# show the figure\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"/kaggle/input/aptos2019-blindness-detection/test_images/351aba543dc8.png","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_array = np.array(Image.open('../input/aptos2019-blindness-detection/test_images/351aba543dc8.png'))\nplt.imshow(img_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize feature maps output from each block in the vgg model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import Model\nfrom matplotlib import pyplot\nfrom numpy import expand_dims\n# load the model\nmodel = VGG16()\n# redefine model to output right after the first hidden layer\nixs = [2, 5, 9, 13, 17]\noutputs = [model.layers[i].output for i in ixs]\nmodel = Model(inputs=model.inputs, outputs=outputs)\n# load the image with the required shape\nimg = load_img('../input/aptos2019-blindness-detection/test_images/351aba543dc8.png', target_size=(224, 224))\n# convert the image to an array\nimg = img_to_array(img)\n# expand dimensions so that it represents a single 'sample'\nimg = expand_dims(img, axis=0)\n# prepare the image (e.g. scale pixel values for the vgg)\nimg = preprocess_input(img)\n# get feature map for first hidden layer\nfeature_maps = model.predict(img)\n# plot the output from each block\nsquare = 4\nfor fmap in feature_maps:\n    # plot all 16 maps in an 4x4 squares\n    ix = 1\n    for _ in range(square):\n        for _ in range(square):\n            # specify subplot and turn of axis\n            # fig, ax = plt.subplots(figsize=(30, 30)) #make images larger for presentation\n            ax = pyplot.subplot(square, square, ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            # plot filter channel in grayscale\n            pyplot.imshow(fmap[0, :, :, ix-1], cmap='bone')\n            ix += 1\n    # show the figure\n    pyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}